Acessar CLI Airflow:
 - Executa "docker ps"
 - Pega o id do webserver ou scheduler
 - executa comando "docker exec -it id /bin/bash
 - estando dentro do airflow, executa o comando exit
 - executar comando "astro dev bash"

Comandos importantes:
#Check if the metadata database can be reached
airflow db check
#Purge old records in database tables in archive table
airflow db clean
#Export archived data from the archive table (default csv)
airflow db export-archived
#Inicialize the database (if using docker or astro, not needed)
airflow db init

#Run subsections of a dag for a specified data range
airflow dags backfill my_dag --reset-dagrun --rerun-failed-tasks --run-backwards -s 2024-01-01 -e 2024-01-10
#Re-sync DAGs
airflow dags reserialize
#List all the dags
airflow dags list

#Test a task instance
airflow tasks test my_dag my_task 2024-01-01

#get list of airflow commands
airflow cheat-sheet

#BASE CONNECTION ON AIRFLOW FOR THIS PROJECT


#HOW TO EXECUTE SPARK:===========
1 - GO TO spark/master FOLDER
2 - EXECUTE COMMAND docker build -t airflow/spark-master . - THIS NAME COMES FROM DOCKER-COMPOESE.OVERRRIDE FILE ON SPARK-MASTER SERVICE
3 - GO TO spark/worker FOLDER
4 - EXECUTE COMMAND docker build -t airflow/spark-worker . - THIS NAME COMES FROM DOCKER-COMPOESE.OVERRRIDE FILE ON SPARK-WORKER SERVICE

# HOW TO EXECUTE AIRFLOW PROJECT:=========
1 - THESE STEPS NEED TO BE DONE AFTER EXECUTING SPARK
2 - GO TO ROOT FILE
3 - EXECUTE ASTRO DEV START


#TEST CONNECTION BETWEEN SERVICES ON DOCKER:========
1 - DOCKER NETWORK LS
2 - DOCKER INSPECT ID

#CONNECTIONS
#https://query1.finance.yahoo.com/
#{
#  "endpoint": "/v8/finance/chart/",
#  "headers": {
#    "Content-Type": "application/json",
#    "User-Agent": "Mozilla/5.0",
#    "Accept": "application/json"
#  }
#}